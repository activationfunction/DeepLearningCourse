{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arbitrary_style_transfer_mnist_svhn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ptran1203/style_transfer/blob/master/Adain_Style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2mpELXM0ABk"
      },
      "source": [
        "Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myR-LFKimeTe"
      },
      "source": [
        "import pickle\r\n",
        "import numpy as np\r\n",
        "import urllib.request\r\n",
        "import keras.preprocessing.image as image_processing\r\n",
        "import cv2\r\n",
        "try:\r\n",
        "    from google.colab.patches import cv2_imshow\r\n",
        "except ImportError:\r\n",
        "    from cv2 import imshow as cv2_imshow\r\n",
        "\r\n",
        "MEAN_PIXCELS = np.array([103.939, 116.779, 123.68])\r\n",
        "\r\n",
        "def pickle_save(object, path, log=False):\r\n",
        "    try:\r\n",
        "        log and print('save data to {} successfully'.format(path))\r\n",
        "        with open(path, \"wb\") as f:\r\n",
        "            return pickle.dump(object, f)\r\n",
        "    except:\r\n",
        "        log and print('save data to {} failed'.format(path))\r\n",
        "\r\n",
        "\r\n",
        "def pickle_load(path, log=False):\r\n",
        "    try:\r\n",
        "        log and print(\"Loading data from {} - \".format(path))\r\n",
        "        with open(path, \"rb\") as f:\r\n",
        "            data = pickle.load(f)\r\n",
        "            log and print(\"DONE\")\r\n",
        "            return data\r\n",
        "    except Exception as e:\r\n",
        "        print(str(e))\r\n",
        "        return None\r\n",
        "\r\n",
        "def norm(imgs):\r\n",
        "    return (imgs - 127.5) / 127.5\r\n",
        "\r\n",
        "\r\n",
        "def de_norm(imgs):\r\n",
        "    return imgs * 127.5 + 127.5\r\n",
        "\r\n",
        "\r\n",
        "def preprocess(imgs):\r\n",
        "    \"\"\"\r\n",
        "    BGR -> RBG then subtract the mean\r\n",
        "    \"\"\"\r\n",
        "    return imgs - MEAN_PIXCELS\r\n",
        "    return imgs[...,[2,1,0]] - MEAN_PIXCELS\r\n",
        "\r\n",
        "\r\n",
        "def deprocess(imgs):\r\n",
        "    return imgs + MEAN_PIXCELS\r\n",
        "    return (imgs + MEAN_PIXCELS)[...,[2,1,0]]\r\n",
        "\r\n",
        "\r\n",
        "def show_images(img_array, denorm=True, deprcs=True):\r\n",
        "    shape = img_array.shape\r\n",
        "    img_array = img_array.reshape(\r\n",
        "        (-1, shape[-4], shape[-3], shape[-2], shape[-1])\r\n",
        "    )\r\n",
        "    # convert 1 channel to 3 channels\r\n",
        "    channels = img_array.shape[-1]\r\n",
        "    resolution = img_array.shape[2]\r\n",
        "    img_rows = img_array.shape[0]\r\n",
        "    img_cols = img_array.shape[1]\r\n",
        "\r\n",
        "    img = np.full([resolution * img_rows, resolution * img_cols, channels], 0.0)\r\n",
        "    for r in range(img_rows):\r\n",
        "        for c in range(img_cols):\r\n",
        "            img[\r\n",
        "            (resolution * r): (resolution * (r + 1)),\r\n",
        "            (resolution * (c % 10)): (resolution * ((c % 10) + 1)),\r\n",
        "            :] = img_array[r, c]\r\n",
        "\r\n",
        "    if denorm:\r\n",
        "        img = de_norm(img)\r\n",
        "    if deprcs:\r\n",
        "        img = deprocess(img)\r\n",
        "\r\n",
        "    cv2_imshow(img)\r\n",
        "\r\n",
        "\r\n",
        "def http_get_img(url, rst=64, gray=False, normalize=True):\r\n",
        "    req = urllib.request.urlopen(url)\r\n",
        "    arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\r\n",
        "    img = cv2.imdecode(arr, -1)\r\n",
        "    if rst is not None:\r\n",
        "        img = image_resize(img, rst)\r\n",
        "    if gray:\r\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n",
        "    \r\n",
        "    img = np.expand_dims(img, 0)\r\n",
        "    if normalize:\r\n",
        "        img = norm(preprocess(img))\r\n",
        "\r\n",
        "    return img\r\n",
        "\r\n",
        "\r\n",
        "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\r\n",
        "    dim = None\r\n",
        "    (h, w) = image.shape[:2]\r\n",
        "    if width is None and height is None:\r\n",
        "        return image\r\n",
        "\r\n",
        "    if width is None:\r\n",
        "        r = height / float(h)\r\n",
        "        dim = (int(w * r), height)\r\n",
        "\r\n",
        "    else:\r\n",
        "        r = width / float(w)\r\n",
        "        dim = (width, int(h * r))\r\n",
        "    resized = cv2.resize(image, dim, interpolation = inter)\r\n",
        "    return resized\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS7uynQc0Ou8"
      },
      "source": [
        "dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqWBM7ohmMzc"
      },
      "source": [
        "import numpy as np\r\n",
        "#import utils\r\n",
        "from collections import Counter\r\n",
        "import os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "try:\r\n",
        "    from google.colab.patches import cv2_imshow\r\n",
        "except ImportError:\r\n",
        "    from cv2 import imshow as cv2_imshow\r\n",
        "\r\n",
        "class DataGenerator:\r\n",
        "    def __init__(self, base_dir, batch_size, rst, max_size=500,\r\n",
        "    multi_batch=False, normalize=True, preprocessing=True):\r\n",
        "        BATCH_FILES = 4\r\n",
        "        self.base_dir = base_dir\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.id = 1\r\n",
        "        self.rst = rst\r\n",
        "        self.multi_batch = multi_batch\r\n",
        "        self.normalize = normalize\r\n",
        "        self.max_size = max_size\r\n",
        "        self.preprocessing = preprocessing\r\n",
        "        self.x = self.get_content_images()\r\n",
        "\r\n",
        "        if multi_batch:\r\n",
        "            self.y = self.get_style_images(self.id)\r\n",
        "        else:\r\n",
        "            self.y = self.get_style_images()\r\n",
        "\r\n",
        "        self.max_size = max_size\r\n",
        "\r\n",
        "        if self.preprocessing:\r\n",
        "            self.x = preprocess(self.x)\r\n",
        "            self.y = preprocess(self.y)\r\n",
        "\r\n",
        "        if normalize:\r\n",
        "            self.x = norm(self.x)\r\n",
        "            self.y = norm(self.y)\r\n",
        "\r\n",
        "\r\n",
        "    def get_content_images(self):\r\n",
        "        return pickle_load(\r\n",
        "            os.path.join(self.base_dir, 'dataset/content_imgs_{}.pkl'.format(self.rst)))[:self.max_size]\r\n",
        "\r\n",
        "\r\n",
        "    def get_style_images(self, _id=\"\"):\r\n",
        "        fname = 'style_imgs_{}'.format(self.rst)\r\n",
        "\r\n",
        "        if _id:\r\n",
        "            fname += \"_\" + str(_id)\r\n",
        "\r\n",
        "        return pickle_load(\r\n",
        "                os.path.join(self.base_dir, 'dataset/{}.pkl'.format(fname)))[:self.max_size]\r\n",
        "\r\n",
        "\r\n",
        "    def next_id(self):\r\n",
        "        self.id += 1\r\n",
        "        if self.id > self.BATCH_FILES:\r\n",
        "            self.id = 1\r\n",
        "        \r\n",
        "        self.y = self.get_style_images(self.id)[:self.max_size]\r\n",
        "\r\n",
        "        if self.preprocessing:\r\n",
        "            self.y = preprocess(self.y)\r\n",
        "        if self.normalize:\r\n",
        "            self.y = norm(self.y)\r\n",
        "\r\n",
        "\r\n",
        "    def augment_one(self, x, y):\r\n",
        "        seed = np.random.randint(0, 100)\r\n",
        "        new_x = transform(x, seed)\r\n",
        "        new_y = transform(y, seed)\r\n",
        "        return new_x, new_y\r\n",
        "\r\n",
        "\r\n",
        "    def augment_array(self, x, y, augment_factor):\r\n",
        "        imgs = []\r\n",
        "        masks = []\r\n",
        "        for i in range(len(x)):\r\n",
        "            imgs.append(x[i])\r\n",
        "            masks.append(y[i])\r\n",
        "            for _ in range(augment_factor):\r\n",
        "                _x, _y = self.augment_one(x[i], y[i])\r\n",
        "                imgs.append(_x)\r\n",
        "                masks.append(_y)\r\n",
        "\r\n",
        "        return np.array(imgs), np.array(masks)\r\n",
        "\r\n",
        "\r\n",
        "    def shuffle_style_imgs(self):\r\n",
        "        size = len(self.y)\r\n",
        "        indices = np.arange(size)\r\n",
        "        np.random.shuffle(indices)\r\n",
        "        return self.y[indices]\r\n",
        "\r\n",
        "\r\n",
        "    def next_batch(self, augment_factor):\r\n",
        "        if self.multi_batch:\r\n",
        "            x = self.x\r\n",
        "            indices = np.arange(x.shape[0])\r\n",
        "            np.random.shuffle(indices)\r\n",
        "            max_id = x.shape[0] - self.batch_size + 1\r\n",
        "            print(\"[\", end=\"\")\r\n",
        "            for i in range(self.BATCH_FILES):\r\n",
        "                for start_idx in range(0, max_id, self.batch_size):\r\n",
        "                    access_pattern = indices[start_idx:start_idx + self.batch_size]\r\n",
        "\r\n",
        "                    yield (\r\n",
        "                        x[access_pattern, :, :, :],\r\n",
        "                        self.y[access_pattern],\r\n",
        "                    )\r\n",
        "                print(\"{}/6 - \".format(i+1), end=\"\")\r\n",
        "                self.next_id()\r\n",
        "            print(\"]\")\r\n",
        "        else:\r\n",
        "            x = self.x\r\n",
        "            self.y = self.shuffle_style_imgs()\r\n",
        "\r\n",
        "            indices = np.arange(x.shape[0])\r\n",
        "            np.random.shuffle(indices)\r\n",
        "            max_id = x.shape[0] - self.batch_size + 1\r\n",
        "            for start_idx in range(0, max_id, self.batch_size):\r\n",
        "                access_pattern = indices[start_idx:start_idx + self.batch_size]\r\n",
        "\r\n",
        "                yield (\r\n",
        "                    x[access_pattern, :, :, :],\r\n",
        "                    self.y[access_pattern],\r\n",
        "                )\r\n",
        "\r\n",
        "    def get_random_sample(self, test=True):\r\n",
        "        if test:\r\n",
        "            idx = np.random.randint(0, self.x_test.shape - 1)\r\n",
        "            return self.x_test[idx], self.y_test[idx]\r\n",
        "\r\n",
        "        idx = np.random.randint(0, self.x.shape - 1)\r\n",
        "        return self.x[idx], self.y[idx]\r\n",
        "\r\n",
        "\r\n",
        "    def random_show(self, option='style'):\r\n",
        "        \"\"\"\r\n",
        "        option: ['style', 'content']\r\n",
        "        \"\"\"\r\n",
        "        idx = np.random.randint(0, self.x.shape - 1)\r\n",
        "        if option == 'style':\r\n",
        "            return cv2_imshow(de_norm(self.y[idx]))\r\n",
        "\r\n",
        "        return cv2_imshow(de_norm(self.x[idx]))\r\n",
        "\r\n",
        "\r\n",
        "    def show_imgs(self, img):\r\n",
        "        if len(img.shape) == 4:\r\n",
        "            return show_images(img, self.normalize, self.preprocessing)\r\n",
        "\r\n",
        "        if self.normalize:\r\n",
        "            img = de_norm(img)\r\n",
        "        if self.preprocessing:\r\n",
        "            img = deprocess(img)\r\n",
        "\r\n",
        "        cv2_imshow(img)\r\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7vrBZ3q0suX"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzEje1k70txF"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import keras\r\n",
        "import numpy as np\r\n",
        "import datetime\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "#import utils\r\n",
        "import keras.backend as K\r\n",
        "\r\n",
        "from keras.layers.convolutional import Conv2D\r\n",
        "from keras.layers import Input, Activation, Layer, UpSampling2D\r\n",
        "from keras.models import Model\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.applications.vgg19 import VGG19\r\n",
        "from keras.applications.vgg16 import VGG16\r\n",
        "\r\n",
        "try:\r\n",
        "    # In case run on google colab\r\n",
        "    from google.colab.patches import cv2_imshow\r\n",
        "except ImportError:\r\n",
        "    from cv2 import imshow as cv2_imshow\r\n",
        "\r\n",
        "DEFAULT_STYLE_LAYERS = [\r\n",
        "    'block1_conv1', 'block2_conv1',\r\n",
        "    'block3_conv1', 'block4_conv1',\r\n",
        "]\r\n",
        "DEFAULT_LAST_LAYER = 'block4_conv1'\r\n",
        "\r\n",
        "\r\n",
        "class AdaptiveInstanceNorm(Layer):\r\n",
        "    def __init__(self, epsilon=1e-3):\r\n",
        "        super(AdaptiveInstanceNorm, self).__init__()\r\n",
        "        self.epsilon = epsilon\r\n",
        "\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        x, style = inputs\r\n",
        "        axis = [1, 2]\r\n",
        "        x_mean = K.mean(x, axis=axis, keepdims=True)\r\n",
        "        x_std = K.std(x, axis=axis, keepdims=True)\r\n",
        "\r\n",
        "        style_mean = K.mean(style, axis=axis, keepdims=True)\r\n",
        "        style_std = K.std(style, axis=axis, keepdims=True)\r\n",
        "\r\n",
        "        norm = (x - x_mean) * (1 / (x_std + self.epsilon))\r\n",
        "\r\n",
        "        return norm * (style_std + self.epsilon) + style_mean\r\n",
        "\r\n",
        "\r\n",
        "    def compute_output_shape(self, input_shape):\r\n",
        "        return input_shape[0]\r\n",
        "\r\n",
        "\r\n",
        "class Reduction(Layer):\r\n",
        "    def __init__(self):\r\n",
        "        super(Reduction, self).__init__()\r\n",
        "\r\n",
        "    def call(self, inputs):\r\n",
        "        return tf.reduce_sum(inputs)\r\n",
        "\r\n",
        "class StyleTransferModel:\r\n",
        "    def __init__(self, base_dir, rst, lr,\r\n",
        "                style_layer_names=DEFAULT_STYLE_LAYERS,\r\n",
        "                last_layer=DEFAULT_LAST_LAYER,\r\n",
        "                show_interval=25,\r\n",
        "                style_loss_weight=1,\r\n",
        "                pre_trained_model='vgg16'):\r\n",
        "        self.base_dir = base_dir\r\n",
        "        self.rst = rst\r\n",
        "        self.pre_trained_model = pre_trained_model\r\n",
        "        self.lr = lr\r\n",
        "        self.style_layer_names = style_layer_names\r\n",
        "        self.last_layer = last_layer\r\n",
        "        self.show_interval = show_interval\r\n",
        "        img_shape = (self.rst, self.rst, 3)\r\n",
        "\r\n",
        "        # ===== Build the model ===== #\r\n",
        "        self.encoder = self.build_encoder()\r\n",
        "        self.style_layers = self.build_style_layers()\r\n",
        "        content_img = Input(shape=img_shape)\r\n",
        "        style_img = Input(shape=img_shape)\r\n",
        "\r\n",
        "        content_feat = self.encoder(content_img)\r\n",
        "        style_feat = self.encoder(style_img)\r\n",
        "\r\n",
        "        combined_feat = AdaptiveInstanceNorm()([content_feat, style_feat])\r\n",
        "        self.init_rst = K.int_shape(combined_feat)[1]\r\n",
        "        self.decoder = self.build_decoder((self.init_rst, self.init_rst, 512))\r\n",
        "\r\n",
        "        gen_img = self.decoder(combined_feat)\r\n",
        "        gen_feat = self.encoder(gen_img)\r\n",
        "\r\n",
        "        self.transfer_model = Model(inputs=[content_img, style_img],\r\n",
        "                                    outputs=gen_img)\r\n",
        "        content_loss = K.mean(K.square(combined_feat - gen_feat), axis=[1, 2])\r\n",
        "        self.transfer_model.add_loss(Reduction()(content_loss))\r\n",
        "        self.transfer_model.add_loss(style_loss_weight*self.compute_style_loss(gen_img, style_img))\r\n",
        "        self.transfer_model.compile(optimizer=Adam(self.lr),\r\n",
        "                                    loss=[\"mse\"],\r\n",
        "                                    loss_weights=[0.0])\r\n",
        "\r\n",
        "\r\n",
        "    def compute_style_loss(self, gen_img, style_img):\r\n",
        "        gen_feats = self.style_layers(gen_img)\r\n",
        "        style_feats = self.style_layers(style_img)\r\n",
        "        style_loss = []\r\n",
        "        axis = [1, 2]\r\n",
        "        for i in range(len(style_feats)):\r\n",
        "            gmean = K.mean(gen_feats[i], axis=axis)\r\n",
        "            gstd = K.std(gen_feats[i], axis=axis)\r\n",
        "\r\n",
        "            smean = K.mean(style_feats[i], axis=axis)\r\n",
        "            sstd = K.std(style_feats[i], axis=axis)\r\n",
        "\r\n",
        "            style_loss.append(\r\n",
        "                K.sum(K.square(gmean - smean)) +\r\n",
        "                K.sum(K.square(gstd - sstd))\r\n",
        "            )\r\n",
        "\r\n",
        "        return Reduction()(style_loss)\r\n",
        "\r\n",
        "\r\n",
        "    def build_style_layers(self):\r\n",
        "        return Model(\r\n",
        "            inputs=self.encoder.inputs,\r\n",
        "            outputs=[self.encoder.get_layer(l).get_output_at(0) \\\r\n",
        "                for l in self.style_layer_names]\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "    def build_encoder(self):\r\n",
        "        input_shape = (self.rst, self.rst, 3)\r\n",
        "        vggnet = VGG16 if self.pre_trained_model == 'vgg16' else VGG19\r\n",
        "        model = vggnet(\r\n",
        "            include_top=False,\r\n",
        "            weights='imagenet',\r\n",
        "            input_tensor=Input(input_shape),\r\n",
        "            input_shape=input_shape,\r\n",
        "        )\r\n",
        "        print('Encoder: {}'.format(model.name))\r\n",
        "        model.trainable = False\r\n",
        "        for layer in model.layers:\r\n",
        "            layer.trainable = False\r\n",
        "\r\n",
        "        return Model(\r\n",
        "            inputs=model.inputs,\r\n",
        "            outputs=model.get_layer(self.last_layer).get_output_at(0)\r\n",
        "        )\r\n",
        "\r\n",
        "\r\n",
        "    def conv_block(self, x, filters, kernel_size,\r\n",
        "                    activation='relu', up_sampling=False):\r\n",
        "\r\n",
        "        x = Conv2D(filters, kernel_size=kernel_size, strides=1,\r\n",
        "                    padding='same', activation=activation)(x)\r\n",
        "\r\n",
        "        if up_sampling:\r\n",
        "            x = UpSampling2D(size=(2, 2), interpolation='nearest')(x)\r\n",
        "\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "    def build_decoder(self, input_shape):\r\n",
        "        feat = Input(input_shape)\r\n",
        "        kernel_size = 3\r\n",
        "\r\n",
        "        x = self.conv_block(feat, 512, kernel_size=kernel_size, up_sampling=True)\r\n",
        "\r\n",
        "        x = self.conv_block(x, 256, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 256, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 256, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 256, kernel_size=kernel_size, up_sampling=True)\r\n",
        "\r\n",
        "        # x = self.conv_block(x, 128, kernel_size=kernel_size)\r\n",
        "        # x = self.conv_block(x, 128, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 128, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 128, kernel_size=kernel_size, up_sampling=True)\r\n",
        "\r\n",
        "        x = self.conv_block(x, 64, kernel_size=kernel_size)\r\n",
        "        x = self.conv_block(x, 64, kernel_size=kernel_size)\r\n",
        "\r\n",
        "        style_image = self.conv_block(x, 3, kernel_size=kernel_size, activation='linear')\r\n",
        "\r\n",
        "        model = Model(inputs=feat, outputs=style_image, name='decoder')\r\n",
        "        return model\r\n",
        "\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def init_hist():\r\n",
        "        return {\r\n",
        "            \"loss\": [],\r\n",
        "            \"val_loss\": []\r\n",
        "        }\r\n",
        "\r\n",
        "\r\n",
        "    def train(self, data_gen, epochs, augment_factor=0):\r\n",
        "        history = self.init_hist()\r\n",
        "        print(\"Train on {} samples\".format(len(data_gen.x)))\r\n",
        "\r\n",
        "        for e in range(epochs):\r\n",
        "            start_time = datetime.datetime.now()\r\n",
        "            print(\"Train epochs {}/{} - \".format(e + 1, epochs), end=\"\")\r\n",
        "\r\n",
        "            batch_loss = self.init_hist()\r\n",
        "            for content_img, style_img in data_gen.next_batch(augment_factor):\r\n",
        "                loss = self.transfer_model.train_on_batch([content_img, style_img],\r\n",
        "                                                          style_img)\r\n",
        "                batch_loss['loss'].append(loss)\r\n",
        "\r\n",
        "            # evaluate\r\n",
        "            # batch_loss['val_loss'] = \r\n",
        "\r\n",
        "            mean_loss = np.mean(np.array(batch_loss['loss']))\r\n",
        "            mean_val_loss = 0#np.mean(np.array(batch_loss['val_loss']))\r\n",
        "\r\n",
        "            history['loss'].append(mean_loss)\r\n",
        "            history['val_loss'].append(mean_val_loss)\r\n",
        "\r\n",
        "            print(\"Loss: {}, Val Loss: {} - {}\".format(\r\n",
        "                mean_loss, mean_val_loss,\r\n",
        "                datetime.datetime.now() - start_time\r\n",
        "            ))\r\n",
        "\r\n",
        "            if e % self.show_interval == 0:\r\n",
        "                self.save_weight()\r\n",
        "                idx = np.random.randint(0, data_gen.max_size - 1)\r\n",
        "                cimg, simg = data_gen.x[idx:idx+1], data_gen.y[idx:idx+1]\r\n",
        "                gen_img = self.generate(cimg, simg)\r\n",
        "                data_gen.show_imgs(np.concatenate([cimg, simg, gen_img]))\r\n",
        "\r\n",
        "        self.history = history\r\n",
        "        return history\r\n",
        "\r\n",
        "\r\n",
        "    def plot_history(self):\r\n",
        "        plt.plot(self.history['loss'], label='train loss')\r\n",
        "        plt.plot(self.history['val_loss'], label='val loss')\r\n",
        "        plt.ylabel('loss')\r\n",
        "        plt.xlabel('epoch')\r\n",
        "        plt.title('Segmentation model')\r\n",
        "        plt.legend()\r\n",
        "        plt.show()\r\n",
        "\r\n",
        "\r\n",
        "    def save_weight(self):\r\n",
        "        try:\r\n",
        "            self.transfer_model.save_weights(self.base_dir + '/transfer_model.h5')\r\n",
        "        except Exception as e:\r\n",
        "            print(\"Could not load model, {}\".format(str(e))) \r\n",
        "\r\n",
        "\r\n",
        "    def load_weight(self):\r\n",
        "        try:\r\n",
        "            self.transfer_model.load_weights(self.base_dir + '/transfer_model.h5')\r\n",
        "        except Exception as e:\r\n",
        "            print(\"Save model failed, {}\".format(str(e))) \r\n",
        "\r\n",
        "\r\n",
        "    def generate(self, content_imgs, style_imgs):\r\n",
        "        return self.transfer_model.predict([content_imgs, style_imgs])\r\n",
        "\r\n",
        "\r\n",
        "    def show_sample(self, content_img, style_img,\r\n",
        "                    concate=True, denorm=True, deprocess=True):\r\n",
        "        gen_img = self.generate(content_img, style_img)\r\n",
        "\r\n",
        "        if concate:\r\n",
        "            return show_images(np.concatenate([content_img, style_img, gen_img]), denorm, deprocess)\r\n",
        "\r\n",
        "        if denorm:\r\n",
        "            content_img = de_norm(content_img)\r\n",
        "            style_img = de_norm(style_img)\r\n",
        "            gen_img = de_norm(gen_img)\r\n",
        "        if deprocess:\r\n",
        "            content_img = deprocess(content_img)\r\n",
        "            style_img = deprocess(style_img)\r\n",
        "            gen_img = deprocess(gen_img)\r\n",
        "\r\n",
        "        cv2_imshow(content_img[0])\r\n",
        "        cv2_imshow(style_img[0])\r\n",
        "        cv2_imshow(gen_img[0])\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7z-woM6V8MW"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from google.colab import drive, output\n",
        "data_loaded = False\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/Style_Transfer\"\n",
        "!rm -rf '/content/style_transfer'\n",
        "!git clone https://github.com/ptran1203/style_transfer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKunvvbIi9NX"
      },
      "source": [
        "BASE_DIR = \"Style_Transfer\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWPPRy7y-0jy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7647ff-fdec-4ccb-a4e5-5024dd68d8b6"
      },
      "source": [
        "cd style_transfer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'style_transfer'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asS8B2UYR4iR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "a02e4399-98e6-4dcc-d62d-61374ca57b54"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "#import utils\n",
        "#from dataloader import DataGenerator\n",
        "#from model import *\n",
        "\n",
        "class DataGen(DataGenerator):\n",
        "    BATCH_FILES= 4\n",
        "\n",
        "class SModel(StyleTransferModel):\n",
        "    pass\n",
        "\n",
        "\n",
        "style_layer_names=[\n",
        "    'block1_conv1', 'block2_conv1',\n",
        "    'block3_conv1', 'block4_conv1',\n",
        "]\n",
        "last_layer='block4_conv1'\n",
        "pre_trained_model = 'vgg19'\n",
        "rst = 256\n",
        "data_gen = DataGen(BASE_DIR, 8, rst=rst, max_size=1500, multi_batch=False,\n",
        "                   normalize=True)\n",
        "smodel = SModel(BASE_DIR, None, 1e-4,\n",
        "                style_layer_names=style_layer_names,\n",
        "                last_layer=last_layer, \n",
        "                show_interval=5,\n",
        "                style_loss_weight=3.5,\n",
        "                pre_trained_model=pre_trained_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'Style_Transfer/dataset/content_imgs_256.pkl'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0c4b99b365a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mrst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m data_gen = DataGen(BASE_DIR, 8, rst=rst, max_size=1500, multi_batch=False,\n\u001b[0;32m---> 25\u001b[0;31m                    normalize=True)\n\u001b[0m\u001b[1;32m     26\u001b[0m smodel = SModel(BASE_DIR, None, 1e-4,\n\u001b[1;32m     27\u001b[0m                 \u001b[0mstyle_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle_layer_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7d5e1075ff91>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base_dir, batch_size, rst, max_size, multi_batch, normalize, preprocessing)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-7d5e1075ff91>\u001b[0m in \u001b[0;36mget_content_images\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_content_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         return pickle_load(\n\u001b[0;32m---> 43\u001b[0;31m             os.path.join(self.base_dir, 'dataset/content_imgs_{}.pkl'.format(self.rst)))[:self.max_size]\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6hAej6NAZU1"
      },
      "source": [
        "smodel.load_weight()\n",
        "smodel.train(data_gen, 500, augment_factor=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcTZh9oyPNXs"
      },
      "source": [
        "urls = [\n",
        "    # 'https://github.com/elleryqueenhomels/arbitrary_style_transfer/raw/master/images/style_thumb/escher_sphere_thumb.jpg',\n",
        "    # 'https://github.com/elleryqueenhomels/arbitrary_style_transfer/raw/master/images/style_thumb/udnie_thumb.jpg',\n",
        "    # 'https://github.com/elleryqueenhomels/arbitrary_style_transfer/raw/master/images/style_thumb/mosaic_thumb.jpg',\n",
        "    # 'https://github.com/elleryqueenhomels/arbitrary_style_transfer/raw/master/images/style_thumb/cat_thumb.jpg'\n",
        "    'https://github.com/lengstrom/fast-style-transfer/blob/master/examples/style/rain_princess.jpg?raw=true',\n",
        "    'https://github.com/lengstrom/fast-style-transfer/blob/master/examples/style/wave.jpg?raw=true',\n",
        "]\n",
        "cimg = utils.http_get_img(\n",
        "    'https://yt3.ggpht.com/a/AATXAJx3V2SYpa27ubB-eIw_vzBgS1QHKcBGj5xAZZ7dQQ=s900-c-k-c0xffffffff-no-rj-mo',\n",
        "    # 'https://github.com/elleryqueenhomels/arbitrary_style_transfer/blob/master/images/content/stata.jpg?raw=true',\n",
        "    512\n",
        ")\n",
        "cv2_imshow(utils.deprocess(utils.de_norm(cimg[0])))\n",
        "for url in urls:\n",
        "    simg = utils.http_get_img(url, 512)\n",
        "    gen = smodel.generate(cimg, simg)\n",
        "    \n",
        "    cv2_imshow(utils.deprocess(utils.de_norm(simg[0])))\n",
        "    cv2_imshow(utils.deprocess(utils.de_norm(gen[0])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}